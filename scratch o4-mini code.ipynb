{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b225be8",
   "metadata": {},
   "source": [
    "## Step 2 - Setup system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d82b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "OPENAI_API = os.getenv('OPENAI_API_KEY')\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\", openai_api_key=OPENAI_API)\n",
    "db_openai = Chroma(persist_directory=\"./vectordb/openai_vectorDB/\", embedding_function=embedding) #for existing database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e8e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container created with ID: cntr_6848012dada4819180050a42566649ab0b9192992cccfdb1\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(api_key=OPENAI_API)\n",
    "cont = llm.containers.create(name=\"test\")\n",
    "\n",
    "container_id_manual = cont.id\n",
    "print(f\"Container created with ID: {container_id_manual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6afa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.responses.response_output_message import ResponseOutputMessage\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def extract_llm_response_code_interpreter(response: list) -> str:\n",
    "    output = \"\"\n",
    "    #print(response)\n",
    "    #print(len(response))\n",
    "    for index in range(len(response)-1, -1, -1):\n",
    "        if isinstance(response[index], ResponseOutputMessage):\n",
    "            #print(len(response[out].content)) check if this length is 1\n",
    "            #print(f\"Output {index}:\\n{response[index].content[0].text}\")\n",
    "            output = response[index].content[0].text\n",
    "            break #stop after the answer is found\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_llm_response_code_interpreter_image(response: list) -> np.ndarray:\n",
    "    # Extract the file citation annotation from the last output message\n",
    "    output_message = response.output[-1].content[0]\n",
    "    annotations = output_message.annotations\n",
    "\n",
    "    # Find the annotation with type 'container_file_citation'\n",
    "    image_annotation = next(\n",
    "        (ann for ann in annotations if getattr(ann, \"type\", \"\") == \"container_file_citation\"),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    if image_annotation is not None:\n",
    "        file_id = image_annotation.file_id\n",
    "        filename = image_annotation.filename\n",
    "        print(f\"Image file ID: {file_id}\")\n",
    "        print(f\"Image filename: {filename}\")\n",
    "\n",
    "        # Check file extension\n",
    "        if filename.lower().endswith(\".png\"):\n",
    "            save_as = filename\n",
    "        elif filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".jpeg\"):\n",
    "            save_as = filename\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file extension. Only .png and .jpg are supported.\")\n",
    "\n",
    "        # Download the file from the container\n",
    "        url = f\"https://api.openai.com/v1/containers/{container_id_manual}/files/{file_id}/content\"\n",
    "        headers = {\"Authorization\": f\"Bearer {OPENAI_API}\"}\n",
    "\n",
    "        response_file = requests.get(url, headers=headers)\n",
    "        if response_file.status_code == 200:\n",
    "            with open(save_as, \"wb\") as f:\n",
    "                f.write(response_file.content)\n",
    "            print(f\"File downloaded and saved as {save_as}\")\n",
    "        else:\n",
    "            print(f\"Failed to download file: {response_file.status_code} - {response_file.text}\")\n",
    "    else:\n",
    "        print(\"No image annotation found in the response.\")\n",
    "    return save_as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8b5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image file ID: cfile_6848013f5a188191861a4bc60e077181\n",
      "Image filename: cfile_6848013f5a188191861a4bc60e077181.png\n",
      "File downloaded and saved as cfile_6848013f5a188191861a4bc60e077181.png\n"
     ]
    }
   ],
   "source": [
    "image = extract_llm_response_code_interpreter_image(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d14d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received: Response(id='resp_68480134339481a2a3f3341476f4d81f018cf5e364e5d963', created_at=1749549364.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='o4-mini-2025-04-16', object='response', output=[ResponseReasoningItem(id='rs_6848013536e481a28a3689a6bf081eeb018cf5e364e5d963', summary=[], type='reasoning', encrypted_content=None, status=None), ResponseCodeInterpreterToolCall(id='ci_6848013a3bfc81a2bbf52f08238aaa2f018cf5e364e5d963', code=\"import matplotlib.pyplot as plt\\n\\n# Data\\ndata = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\\n\\n# Create histogram\\nplt.figure(figsize=(6, 4))\\nplt.hist(data, bins=[0.5, 1.5, 2.5, 3.5, 4.5], edgecolor='black')\\nplt.xticks([1, 2, 3, 4])\\nplt.xlabel('Value')\\nplt.ylabel('Frequency')\\nplt.title('Histogram of Given Data')\\nplt.grid(axis='y', alpha=0.75)\\nplt.show()\", results=None, status='completed', type='code_interpreter_call', container_id='cntr_6848012dada4819180050a42566649ab0b9192992cccfdb1', outputs=None), ResponseOutputMessage(id='msg_6848013ff66c81a2bf5b509f261a3a13018cf5e364e5d963', content=[ResponseOutputText(annotations=[AnnotationContainerFileCitation(container_id='cntr_6848012dada4819180050a42566649ab0b9192992cccfdb1', end_index=0, file_id='cfile_6848013f5a188191861a4bc60e077181', start_index=0, type='container_file_citation', filename='cfile_6848013f5a188191861a4bc60e077181.png')], text='Here is the histogram of your data [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]. Each bar shows the frequency of each integer value. Let me know if you’d like any adjustments (e.g., different binning, colors, annotations).', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[CodeInterpreter(container='cntr_6848012dada4819180050a42566649ab0b9192992cccfdb1', type='code_interpreter')], top_p=1.0, background=False, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=2734, input_tokens_details=InputTokensDetails(cached_tokens=804), output_tokens=606, output_tokens_details=OutputTokensDetails(reasoning_tokens=384), total_tokens=3340), user=None, store=True)\n",
      "Image file ID: cfile_6848013f5a188191861a4bc60e077181\n",
      "Error processing question in container\n",
      "Error: name 'filename' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.responses.create(\n",
    "        model=\"o4-mini\",\n",
    "        tools=[{\"type\": \"code_interpreter\", \"container\": container_id_manual}],\n",
    "        #instructions=custom_prompt, #disabled the prompt for now, we could include it here later and directly feed the exam question\n",
    "        #but this approach might not work with the Streamlit app\n",
    "        input=\"Create a histogram of the following data: [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\",\n",
    "    )\n",
    "    # Extract the answer from the response\n",
    "    print(f\"Response received: {response}\")\n",
    "    answer = extract_llm_response_code_interpreter(response.output)\n",
    "    image = extract_llm_response_code_interpreter_image(response)\n",
    "    if answer == \"\":\n",
    "        raise ValueError(\"Empty answer received from LLM.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing question in container\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d8de28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseOutputText(annotations=[AnnotationContainerFileCitation(container_id='cntr_6847f97337cc8191ac0f3788d0cd05750a5731a572001f38', end_index=0, file_id='cfile_6847f9833d8081918800fef81d1581a2', start_index=0, type='container_file_citation', filename='cfile_6847f9833d8081918800fef81d1581a2.png')], text=\"Here's the histogram of your data:\\n\\n- Value 1 appears once.\\n- Value 2 appears twice.\\n- Value 3 appears three times.\\n- Value 4 appears four times.\\n\\nThe x-axis shows the data values and the y-axis shows their frequencies. Let me know if you’d like any changes or further analysis!\", type='output_text', logprobs=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image file ID: cfile_6847f9833d8081918800fef81d1581a2\n",
      "File downloaded and saved as cfile_6847f9833d8081918800fef81d1581a2.png\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4447bf29",
   "metadata": {},
   "source": [
    "## Step 3 - Evaluate with system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1439c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_llm_answer_openai(exam_question, prompt, openai_model, use_RAG=False, k=4, container_id=None) -> str:\n",
    "    if use_RAG == True:\n",
    "        if k == 0:\n",
    "            return \"Error retrieving\"\n",
    "        # Retrieve relevant context from the vector database\n",
    "        retrieved_docs = db_openai.similarity_search(exam_question, k=k)\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "        rag_prompt = (\n",
    "            \"Use the following pieces of retrieved context to answer the question. \"\n",
    "            \"\\n\\nContext:\\n\" + context\n",
    "        )\n",
    "        prompt = prompt + rag_prompt\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": exam_question})\n",
    "    if container_id is not None:\n",
    "        #print(f\"Using container ID: {container_id}\")\n",
    "        try:\n",
    "            response = llm.responses.create(\n",
    "                model=openai_model,\n",
    "                tools=[{\"type\": \"code_interpreter\", \"container\": container_id}],\n",
    "                #instructions=custom_prompt, #disabled the prompt for now, we could include it here later and directly feed the exam question\n",
    "                #but this approach might not work with the Streamlit app\n",
    "                input=messages\n",
    "            )\n",
    "            # Extract the answer from the response\n",
    "            answer = extract_llm_response_code_interpreter(response.output)\n",
    "            if answer == \"\":\n",
    "                raise ValueError(\"Empty answer received from LLM.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question in container: {exam_question}\\nError: {e}\")\n",
    "            answer = get_llm_answer_openai(exam_question, prompt, openai_model, use_RAG, k=k-1, container_id=container_id)\n",
    "    else:\n",
    "        try:\n",
    "            response = llm.chat.completions.create(\n",
    "                model=openai_model,\n",
    "                messages=messages,\n",
    "            )\n",
    "            answer = response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {exam_question}\\nError: {e}\")\n",
    "            answer = get_llm_answer_openai(exam_question, prompt, openai_model, use_RAG, k=k-1)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def get_llm_answers_openai(exam_questions_TF, prompt, openai_model, use_RAG=False, container_id=None) -> list[str]:\n",
    "    llm_answers_TF = []\n",
    "    for question in tqdm(exam_questions_TF):\n",
    "        llm_answers_TF.append(get_llm_answer_openai(question, prompt, openai_model, use_RAG=use_RAG, container_id=container_id))\n",
    "    return llm_answers_TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e46e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_llm(llm_answers: list[str]) -> list[str]:\n",
    "    cleaned_answers = []\n",
    "    for org_answer in llm_answers:\n",
    "        answer = org_answer.lower()\n",
    "        if \"true\" in answer and \"false\" in answer:\n",
    "            #print(\"Case 1\")\n",
    "            # Check if one is bold that overrules the other\n",
    "            # For example, an answer is bold, and there is \"not true\" in the answer too\n",
    "            if \"\\\\textbf{false}\" in answer and \"\\\\textbf{true}\" in answer:\n",
    "                print(\"Warning: Both True and False found in the answer, CONFLICT.\")\n",
    "                print(\"Answer:\", answer)\n",
    "                print(\"Index:\", llm_answers.index(org_answer))\n",
    "                print(40* \"-\")\n",
    "                cleaned_answers.append(\"CONFLICT\")\n",
    "            # Give priority to bold answer or final conclusion in the beginning of the answer\n",
    "            elif \"\\\\textbf{false}\" in answer or \"false\" in answer[:5]: \n",
    "                cleaned_answers.append(\"False\")\n",
    "            elif \"\\\\textbf{true}\" in answer or \"true\" in answer[:5]:\n",
    "                cleaned_answers.append(\"True\")\n",
    "            else:\n",
    "                print(\"Warning: unchecked case.\")\n",
    "                print(\"Answer:\", answer)\n",
    "                print(\"Index:\", llm_answers.index(org_answer))\n",
    "                print(40* \"-\")\n",
    "                cleaned_answers.append(\"CONFLICT\")\n",
    "        else:\n",
    "            #print(\"Case 2\")\n",
    "            if \"true\" in answer or \"\\\\textbf{true}\" in answer:\n",
    "                cleaned_answers.append(\"True\")\n",
    "            elif \"false\" in answer or \"\\\\textbf{False}\" in answer:\n",
    "                cleaned_answers.append(\"False\")\n",
    "            else:\n",
    "                print(\"Warning: Neither True nor False found in the answer, CONFLICT.\")\n",
    "                print(\"Answer:\", answer)\n",
    "                print(\"Index:\", llm_answers.index(org_answer))\n",
    "                print(40* \"-\")\n",
    "                cleaned_answers.append(\"CONFLICT\")\n",
    "    return cleaned_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c895615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answers_Martijn(true_answers: list[str]) -> list[str]:\n",
    "    cleaned_answers = []\n",
    "    for org_answer in true_answers:\n",
    "        answer = org_answer.lower()\n",
    "        if \"\\\\textbf{false}\" in answer:\n",
    "            cleaned_answers.append(\"False\")\n",
    "        elif \"\\\\textbf{true}\" in answer:\n",
    "            cleaned_answers.append(\"True\")\n",
    "        else:\n",
    "            print(\"Warning: Neither True nor False found in the Martijn's answer, CONFLICT.\")\n",
    "            print(\"Answer:\", answer)\n",
    "            print(\"Index:\", true_answers.index(org_answer))\n",
    "            print(\"CONFLICT MARTIJN\")\n",
    "        #cleaned_answers.append(\"True\")\n",
    "    return cleaned_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef43599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(Martijn_answers: list[str], llm_answers: list[str]) -> float:\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for Martijn_answer, llm_answer in zip(Martijn_answers, llm_answers):\n",
    "        if Martijn_answer.lower() == llm_answer.lower():\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    total = correct + incorrect\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeb89d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_dataframe(questions: list[str], llm_answers: list[str], Martijn_answers: list[str]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame({\n",
    "        'Question': questions,\n",
    "        'LLM Answer': llm_answers,\n",
    "        'Martijn Answer': Martijn_answers\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f18d6",
   "metadata": {},
   "source": [
    "### Step 3.1 - Baseline LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dfb5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = \"o4-mini\" #o4-mini-2025-04-16\n",
    "custom_prompt = (\n",
    "    \"You are an assistant for question-answering tasks in linear algebra. \"\n",
    "    \"Your are given a True/False statement. You must include 'True', 'False' or 'I don't know' in your answer. \"\n",
    "    \"If the statement is 'False', a counter-example is sufficient. \"\n",
    "    \"If the statement is 'True', you briefly outline a proof and/or mention relevant theorems. \"\n",
    "    \"If you are not sure, you say 'I don't know'. \"\n",
    "    \"Please use LaTeX formatting for mathematical expressions by writing them between dollar signs.\"\n",
    "    \"For example, to write a matrix, use $\\\\begin{pmatrix} a & b \\\\\\\\ c & d \\\\end{pmatrix}$. \" \n",
    "    \"You can write and run code to answer the question. \"\n",
    ")\n",
    "custom_prompt_construction = (\n",
    "    \"You are an assistant for question-answering tasks in linear algebra. \"\n",
    "    \"Your are given a question to construct. \"\n",
    "    \"If you do not know the answer, respond with 'I don't know'. \"\n",
    "    \"Please use LaTeX formatting for mathematical expressions by writing them between dollar signs.\"\n",
    "    \"For example, to write a matrix, use $\\\\begin{pmatrix} a & b \\\\\\\\ c & d \\\\end{pmatrix}$. \"\n",
    "    \"You can write and run code to answer the question. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955241b",
   "metadata": {},
   "source": [
    "True/False questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d32ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_llm_answers_openai([exam_questions_TF[0]], custom_prompt, openai_model, container_id=container_id_manual) #single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19dd797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [09:28<00:00,  6.77s/it]\n"
     ]
    }
   ],
   "source": [
    "llm_answers_TF = get_llm_answers_openai(exam_questions_TF, custom_prompt, openai_model, container_id=container_id_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9af3311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LLM answers compared to Martijn's answers: 92.86%\n"
     ]
    }
   ],
   "source": [
    "llm_answers_cleaned = extract_answer_llm(llm_answers_TF)\n",
    "Martijn_answers = extract_answers_Martijn(exam_answers_TF)\n",
    "\n",
    "assert len(llm_answers_cleaned) == len(Martijn_answers), \"Mismatch between LLM answers and Martijn's answers.\"\n",
    "\n",
    "accuracy = compute_accuracy(Martijn_answers, llm_answers_cleaned)\n",
    "print(f\"Accuracy of LLM answers compared to Martijn's answers: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39c00969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_o4mini_CODE_TF = convert_dataframe(\n",
    "    exam_questions_TF,\n",
    "    llm_answers_TF,\n",
    "    exam_answers_TF\n",
    ")\n",
    "df_baseline_o4mini_CODE_TF.to_pickle(\"results/o4-mini-code/o4mini_TF_CODE_3.pkl\")\n",
    "#temp = pd.read_pickle(\"results/o4-mini-code/o4mini_TF_CODE_3.pkl\")\n",
    "#df_baseline_o4mini_CODE_TF.to_csv(\"results/o4-mini-code/o4mini_TF_CODE_3.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776c18d",
   "metadata": {},
   "source": [
    "Construction questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f7e9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    llm_answers_construction = get_llm_answers_openai(exam_answers_construction, custom_prompt_construction, openai_model, container_id=container_id_manual)\n",
    "\n",
    "    df_baseline_o4mini_construction = convert_dataframe(\n",
    "        exam_questions_construction,\n",
    "        llm_answers_construction,\n",
    "        exam_answers_construction\n",
    "    )\n",
    "    df_baseline_o4mini_construction.to_pickle(\"results/o4-mini-code/o4mini_CODE_Construction.pkl\")\n",
    "    #temp = pd.read_pickle(\"results/o4-mini-code/o4mini_CODE_Construction.pkl\")\n",
    "    #df_baseline_o4mini_construction.to_csv(\"results/o4-mini-code/o4mini_CODE_Construction.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97035ca0",
   "metadata": {},
   "source": [
    "### Step 3.2 - Baseline LLM + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db7e7ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = \"o4-mini\" #o4-mini-2025-04-16\n",
    "custom_prompt = (\n",
    "    \"You are an assistant for question-answering tasks in linear algebra. \"\n",
    "    \"Your are given a True/False statement. You must include 'True', 'False' or 'I don't know' in your answer. \"\n",
    "    \"If the statement is 'False', a counter-example is sufficient. \"\n",
    "    \"If the statement is 'True', you briefly outline a proof and/or mention relevant theorems. \"\n",
    "    \"If you are not sure, you say 'I don't know'. \"\n",
    "    \"Please use LaTeX formatting for mathematical expressions by writing them between dollar signs.\"\n",
    "    \"For example, to write a matrix, use $\\\\begin{pmatrix} a & b \\\\\\\\ c & d \\\\end{pmatrix}$. \" \n",
    "    \"You can write and run code to answer the question. \"\n",
    ")\n",
    "custom_prompt_construction = (\n",
    "    \"You are an assistant for question-answering tasks in linear algebra. \"\n",
    "    \"Your are given a question to construct. \"\n",
    "    \"If you do not know the answer, respond with 'I don't know'. \"\n",
    "    \"Please use LaTeX formatting for mathematical expressions by writing them between dollar signs.\"\n",
    "    \"For example, to write a matrix, use $\\\\begin{pmatrix} a & b \\\\\\\\ c & d \\\\end{pmatrix}$. \"\n",
    "    \"You can write and run code to answer the question. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a485df8a",
   "metadata": {},
   "source": [
    "True/False questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c7ff758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [10:34<00:00,  7.56s/it]\n"
     ]
    }
   ],
   "source": [
    "llm_answers_TF = get_llm_answers_openai(exam_questions_TF, custom_prompt, openai_model, use_RAG=True, container_id=container_id_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85cf1906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains \"Error retrieving\": False\n"
     ]
    }
   ],
   "source": [
    "error_present = any(\"Error retrieving\" in answer for answer in llm_answers_TF)\n",
    "print(f'Contains \"Error retrieving\": {error_present}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83026601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LLM answers compared to Martijn's answers: 96.43%\n"
     ]
    }
   ],
   "source": [
    "llm_answers_cleaned = extract_answer_llm(llm_answers_TF)\n",
    "Martijn_answers = extract_answers_Martijn(exam_answers_TF)\n",
    "\n",
    "accuracy = compute_accuracy(Martijn_answers, llm_answers_cleaned)\n",
    "print(f\"Accuracy of LLM answers compared to Martijn's answers: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a5d4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_o4mini_RAG_CODE_TF = convert_dataframe(\n",
    "    exam_questions_TF,\n",
    "    llm_answers_TF,\n",
    "    exam_answers_TF\n",
    ")\n",
    "df_baseline_o4mini_RAG_CODE_TF.to_pickle(\"results/o4-mini-code/o4mini_RAG_CODE_TF_3.pkl\")\n",
    "#temp = pd.read_pickle(\"results/o4-mini-code/o4mini_RAG_CODE_TF_3.pkl\")\n",
    "#df_baseline_o4mini_RAG_CODE_TF.to_csv(\"results/o4-mini-code/o4mini_RAG_CODE_TF_3.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa71b1",
   "metadata": {},
   "source": [
    "Construction questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1264fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    llm_answers_construction = get_llm_answers_openai(exam_answers_construction, custom_prompt_construction, openai_model, use_RAG=True, container_id=container_id_manual)\n",
    "\n",
    "    df_baseline_o4mini_RAG_construction = convert_dataframe(\n",
    "        exam_questions_construction,\n",
    "        llm_answers_construction,\n",
    "        exam_answers_construction\n",
    "    )\n",
    "    df_baseline_o4mini_RAG_construction.to_pickle(\"results/o4-mini-code/o4mini_RAG_CODE_Construction.pkl\")\n",
    "    #temp = pd.read_pickle(\"results/o4-mini-code/o4mini_RAG_CODE_Construction.pkl\")\n",
    "    #df_baseline_o4mini_RAG_construction.to_csv(\"results/o4-mini-code/o4mini_RAG_CODE_Construction.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8ee0a",
   "metadata": {},
   "source": [
    "### Step 4.1 - Baseline LLM + all-in-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c2dcd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load the JSON file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): name of the file to load\n",
    "\n",
    "    Returns:\n",
    "        dict: json file content as a dictionary\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        file = json.load(f)\n",
    "    return file\n",
    "\n",
    "topics = load_json('topics.json')['Topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f35faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65 theorems.\n"
     ]
    }
   ],
   "source": [
    "theorems = []\n",
    "for topic in topics.values():\n",
    "    for section, items in topic.items():\n",
    "        for item in items:\n",
    "            if isinstance(item, dict) and \"metadata\" in item:\n",
    "                if item[\"metadata\"].get(\"type\") == \"theorem\":\n",
    "                    theorems.append(item)\n",
    "print(f\"There are {len(theorems)} theorems.\")\n",
    "theorems_text = [theorem['text'] for theorem in theorems] #only get the text of the theorems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcf94275",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = \"o4-mini\" #o4-mini-2025-04-16\n",
    "custom_prompt = (\n",
    "    \"You are an assistant for question-answering tasks in linear algebra. \"\n",
    "    \"Your are given a True/False statement. You must include 'True', 'False' or 'I don't know' in your answer. \"\n",
    "    \"If the statement is 'False', a counter-example is sufficient. \"\n",
    "    \"If the statement is 'True', you briefly outline a proof and/or mention relevant theorems. \"\n",
    "    \"If you are not sure, you say 'I don't know'. \"\n",
    "    \"Please use LaTeX formatting for mathematical expressions by writing them between dollar signs.\"\n",
    "    \"For example, to write a matrix, use $\\\\begin{pmatrix} a & b \\\\\\\\ c & d \\\\end{pmatrix}$. \"\n",
    "    \"You can write and run code to answer the question. \"\n",
    "    \"You can use the following theorems to answer the question. \" + \"\\n\\n\".join(theorems_text)\n",
    ")\n",
    "custom_prompt_construction = (\n",
    "    \"You are an assistant for question-answering tasks in linear algebra. \"\n",
    "    \"Your are given a question to construct. \"\n",
    "    \"If you do not know the answer, respond with 'I don't know'. \"\n",
    "    \"Please use LaTeX formatting for mathematical expressions by writing them between dollar signs.\"\n",
    "    \"For example, to write a matrix, use $\\\\begin{pmatrix} a & b \\\\\\\\ c & d \\\\end{pmatrix}$. \"\n",
    "    \"You can write and run code to answer the question. \"\n",
    "    \"You can use the following theorems to answer the question. \" + \"\\n\\n\".join(theorems_text)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4b5cb",
   "metadata": {},
   "source": [
    "True/False questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9235f7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [11:56<00:00,  8.53s/it]\n"
     ]
    }
   ],
   "source": [
    "llm_answers_TF = get_llm_answers_openai(exam_questions_TF, custom_prompt, openai_model, container_id=container_id_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a5521d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LLM answers compared to Martijn's answers: 94.05%\n"
     ]
    }
   ],
   "source": [
    "llm_answers_cleaned = extract_answer_llm(llm_answers_TF)\n",
    "Martijn_answers = extract_answers_Martijn(exam_answers_TF)\n",
    "\n",
    "accuracy = compute_accuracy(Martijn_answers, llm_answers_cleaned)\n",
    "print(f\"Accuracy of LLM answers compared to Martijn's answers: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd8e665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_o4mini_PROMPT_CODE_TF = convert_dataframe(\n",
    "    exam_questions_TF,\n",
    "    llm_answers_TF,\n",
    "    exam_answers_TF\n",
    ")\n",
    "df_baseline_o4mini_PROMPT_CODE_TF.to_pickle(\"results/o4-mini-code/o4mini_PROMPT_CODE_TF_3.pkl\")\n",
    "#temp = pd.read_pickle(\"results/o4-mini-code/o4mini_PROMPT_CODE_TF_3.pkl\")\n",
    "#df_baseline_o4mini_PROMPT_CODE_TF.to_csv(\"results/o4-mini-code/o4mini_PROMPT_CODE_TF_3.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4690f",
   "metadata": {},
   "source": [
    "Construction questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbc89c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    llm_answers_construction = get_llm_answers_openai(exam_answers_construction, custom_prompt_construction, openai_model, container_id=container_id_manual)\n",
    "    \n",
    "    df_baseline_o4mini_PROMPT_construction = convert_dataframe(\n",
    "        exam_questions_construction,\n",
    "        llm_answers_construction,\n",
    "        exam_answers_construction\n",
    "    )\n",
    "    df_baseline_o4mini_PROMPT_construction.to_pickle(\"results/o4-mini-code/o4mini_PROMPT_CODE_Construction.pkl\")\n",
    "    #temp = pd.read_pickle(\"results/o4-mini-code/o4mini_PROMPT_CODE_Construction.pkl\")\n",
    "    #df_baseline_o4mini_PROMPT_construction.to_csv(\"results/o4-mini-code/o4mini_PROMPT_CODE_Construction.csv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
