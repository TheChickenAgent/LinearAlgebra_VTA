{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f3f42e",
   "metadata": {},
   "source": [
    "# OpenAI notebook\n",
    "\n",
    "| Pedagogy Dimension | Metrics |\n",
    "| --- | ----------- |\n",
    "| Manage cognitive load | Stay on topic |\n",
    "| Encourage active learning | Do not reveal the answer; guide towards the answer; promote active engagement |\n",
    "| Deepen metacognition | Identify and address misconceptions |\n",
    "| Motivate and stimulate curiosity | Communicate with positive tone; respond appropriately to explicit affect cues |\n",
    "| Adapt to the learners’ goals and needs | Adapt to the learner’s level |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15f9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "DATA_DIR = \"user-testing/data/\"\n",
    "\n",
    "def list_pickle_files(directory):\n",
    "    return [f for f in os.listdir(directory) if f.endswith('.pkl')]\n",
    "\n",
    "def load_conversation(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        return data\n",
    "    \n",
    "def read_questions_answers():\n",
    "    # Split out True/False questions\n",
    "    exam_questions_TF = []\n",
    "    exam_answers_TF = []\n",
    "\n",
    "    with open('exams/together2.tex', 'r', encoding='utf-8') as file:\n",
    "        tex_content = file.read()\n",
    "        questions, answers = extract_question_answer(tex_content)\n",
    "        #print(f\"Total Questions: {len(questions)}\")\n",
    "        #print(f\"Total Answers: {len(answers)}\")\n",
    "        if len(questions) != len(answers):\n",
    "            print(\"Warning: The number of questions and answers do not match!\")\n",
    " \n",
    "        #print()\n",
    "        for idx, (q, a) in enumerate(zip(questions, answers), 1):\n",
    "            #print(f\"Question {idx}:\\n{q}\\n\")\n",
    "            #print(f\"Answer {idx}:\\n{a}\\n\")\n",
    "            exam_questions_TF.append(q)\n",
    "            exam_answers_TF.append(a)\n",
    "    return exam_questions_TF, exam_answers_TF\n",
    "\n",
    "def extract_question_answer(tex_content):\n",
    "    # Extract content within the enumerate environment\n",
    "    enum_match = re.search(r'\\\\begin{enumerate}(.*?)\\\\end{enumerate}', tex_content, re.DOTALL)\n",
    "    if not enum_match:\n",
    "        return [], []\n",
    "    enum_content = enum_match.group(1)\n",
    "\n",
    "    # Find all questions (\\item ... \\begin{solutionorbox})\n",
    "    question_blocks = re.findall(\n",
    "        r'\\\\item(.*?)(?=\\\\begin{solutionorbox})',\n",
    "        enum_content, re.DOTALL\n",
    "    )\n",
    "\n",
    "    # Find all answers (\\begin{solutionorbox} ... \\end{solutionorbox})\n",
    "    answer_blocks = re.findall(\n",
    "        r'\\\\begin{solutionorbox}\\[[^\\]]*\\]\\s*(.*?)\\\\end{solutionorbox}',\n",
    "        enum_content, re.DOTALL\n",
    "    )\n",
    "    questions = [q.strip() for q in question_blocks]\n",
    "    answers = [a.strip() for a in answer_blocks]\n",
    "    return questions, answers\n",
    "\n",
    "def remove_latex_formatting(question: str) -> str:\n",
    "    # Remove LaTeX bold formatting\n",
    "    question = question.replace(\"\\\\textbf{always}\", \"*always*\")  \n",
    "    question = question.replace(\"\\\\textbf{Every}\", \"*Every*\")\n",
    "    question = question.replace(\"\\\\textbf{any}\", \"*any*\")\n",
    "    question = question.replace(\"\\\\textbf{rotation}\", \"*rotation*\")\n",
    "    question = question.replace(\"\\\\textbf{distinct}\", \"*distinct*\")\n",
    "\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4d1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "OPENAI_API = os.getenv('OPENAI_API_KEY')\n",
    "llm = OpenAI(api_key=OPENAI_API)\n",
    "openai_model = \"o4-mini\"\n",
    "\n",
    "o4_llm_state = pickle.load(open('exams/together2_llm_state.pkl', 'rb'))\n",
    "exam_questions_TF, exam_answers_TF = read_questions_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafa5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_summary = \"\"\"\n",
    "You are an AI assistant that summarizes conversations between a Student and a Tutor AI model.\n",
    "Your task is to analyze the conversation and provide a summary of the student's main difficulties.\n",
    "You are not responsible for providing answers to the questions, but rather to identify the key areas where the student struggled or needed help.\n",
    "You will be given a conversation in the form of a list of messages, where each message is\n",
    "a dictionary with 'role' (either 'user' or 'assistant') and 'content' (the text of the message).\n",
    "Your summary should be concise and focus on the student's challenges, misconceptions, or areas of confusion.\n",
    "***The conversation:***\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f669a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_summary = \"\"\"\n",
    "You are an AI assistant that merges the difficulties that students had during a conversation with a Tutor AI model.\n",
    "You will be given a string with all the difficulties for the same (fixed) question.\n",
    "Your task is to summarize/merge the different difficulties that students had during the conversation and provide a summary of the student's main difficulties.\n",
    "Your summary should be concise and focus on the student's challenges, misconceptions, or areas of confusion.\n",
    "***All difficulties:***\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b830f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_summary = \"\"\"\n",
    "You are an AI assistant that merges the difficulties that students had during a conversation with a Tutor AI model.\n",
    "You will be given a string with all the difficulties for different questions.\n",
    "Your task is to summarize/merge the different difficulties that students had during the conversation and provide a summary of the student's main difficulties.\n",
    "Your summary should be concise and focus on the student's challenges, misconceptions, or areas of confusion.\n",
    "***All difficulties over different questions:***\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe0b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_role_prefix_to_conversation(conversation):\n",
    "    \"\"\"\n",
    "    Returns a new conversation list where each message's content is prefixed\n",
    "    with 'Student:' or 'Tutor:' depending on the role.\n",
    "\n",
    "    Args:\n",
    "        conversation (list): List of dicts with 'role' and 'content'.\n",
    "\n",
    "    Returns:\n",
    "        list: New conversation list with prefixed content.\n",
    "    \"\"\"\n",
    "    role_prefix = {'user': 'Student:', 'assistant': 'Tutor:'}\n",
    "    new_conversation = []\n",
    "    for msg in conversation:\n",
    "        prefix = role_prefix.get(msg['role'], '')\n",
    "        new_content = f\"{prefix} {msg['content'].strip()}\"\n",
    "        new_conversation.append({'role': msg['role'], 'content': new_content})\n",
    "    return new_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45005dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(prompt: str, messages:list|str, openai_model=\"o4-mini\"):\n",
    "    \"\"\"\n",
    "    Query the LLM with the given prompt and messages.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the LLM.\n",
    "        messages (list): List of messages in the conversation.\n",
    "        openai_model (str): The OpenAI model to use for the query.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the LLM.\n",
    "    \"\"\"\n",
    "    new_messages = []\n",
    "    if isinstance(messages, list):\n",
    "        new_messages.append({\"role\": \"system\", \"content\": prompt})\n",
    "\n",
    "        for msg in messages:\n",
    "            new_messages.append({\n",
    "                \"role\": msg[\"role\"],\n",
    "                \"content\": msg[\"content\"]\n",
    "            })\n",
    "    elif isinstance(messages, str):\n",
    "        new_messages.append({\"role\": \"system\", \"content\": prompt})\n",
    "        new_messages.append({\"role\": \"user\", \"content\": messages})\n",
    "    new_messages.append({\"role\": \"system\", \"content\": \"Request: perform your task.\"})\n",
    "    try:\n",
    "        response = llm.chat.completions.create(\n",
    "            model=openai_model,\n",
    "            messages=new_messages,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c7df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report_ideas(openai_model=\"o4-mini\"):\n",
    "    files_questions = {}\n",
    "    for filename in list_pickle_files(DATA_DIR):\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        conversation_data = load_conversation(filepath)\n",
    "\n",
    "        # Get the selected question\n",
    "        selected_question = int(conversation_data[\"selected_question\"].strip(\"Q\")) - 1\n",
    "\n",
    "        # Store it in the dictionary\n",
    "        if selected_question in files_questions:\n",
    "            files_questions[selected_question].append(filepath)\n",
    "        else:\n",
    "            files_questions[selected_question] = [filepath]\n",
    "            \n",
    "\n",
    "    # Now, we can process each question with its associated files\n",
    "    recommendations_per_file = {}\n",
    "    for selected_question, files in tqdm(files_questions.items()):\n",
    "        for file in tqdm(files):\n",
    "            conversation_data = load_conversation(file)\n",
    "            conversation = conversation_data[\"messages\"]\n",
    "            student_tutor_conversations = add_role_prefix_to_conversation(conversation)\n",
    "\n",
    "            # Use LLM to get recommendations for each conversation\n",
    "            recommendation = query_llm(conversation_summary, student_tutor_conversations, openai_model=openai_model)\n",
    "\n",
    "            # Store it in the dictionary\n",
    "            if selected_question in recommendations_per_file:\n",
    "                recommendations_per_file[selected_question].append(recommendation)\n",
    "            else:\n",
    "                recommendations_per_file[selected_question] = [recommendation]\n",
    "\n",
    "    # Merge the recommendations into a single list per question\n",
    "    recommendations_per_question = {}\n",
    "    for selected_question in tqdm(recommendations_per_file.keys()):\n",
    "        recommendations = recommendations_per_file[selected_question]\n",
    "        merged_recommendation = \"\\n\\n\".join(recommendations)\n",
    "\n",
    "        # Use LLM to get merged recommendation for each question.\n",
    "        merged_recommendation = query_llm(question_summary, merged_recommendation, openai_model=openai_model)\n",
    "\n",
    "        recommendations_per_question[selected_question] = merged_recommendation\n",
    "    \n",
    "    # Get overall recommendations\n",
    "    merged_overall_recommendations = \" \".join(recommendations_per_question.values())\n",
    "    # Use LLM to get overall recommendations\n",
    "    overall_recommendations = query_llm(overall_summary, merged_overall_recommendations, openai_model=openai_model)\n",
    "\n",
    "    return recommendations_per_file, recommendations_per_question, overall_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6376bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recommendations_per_file, recommendations_per_question, overall_recommendations = get_report_ideas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "366f9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_questions = {}\n",
    "for filename in list_pickle_files(DATA_DIR):\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    conversation_data = load_conversation(filepath)\n",
    "\n",
    "    # Get the selected question\n",
    "    selected_question = int(conversation_data[\"selected_question\"].strip(\"Q\")) - 1\n",
    "\n",
    "    # Store it in the dictionary\n",
    "    if selected_question in files_questions:\n",
    "        files_questions[selected_question].append(filepath)\n",
    "    else:\n",
    "        files_questions[selected_question] = [filepath]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a58e7485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:20<00:00,  5.13s/it]\n",
      "100%|██████████| 6/6 [00:29<00:00,  4.90s/it]\n",
      "100%|██████████| 3/3 [00:12<00:00,  4.18s/it]\n",
      "100%|██████████| 6/6 [00:29<00:00,  4.84s/it]\n",
      "100%|██████████| 10/10 [00:43<00:00,  4.32s/it]\n",
      "100%|██████████| 5/5 [00:20<00:00,  4.15s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.86s/it]\n",
      "100%|██████████| 7/7 [02:40<00:00, 22.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now, we can process each question with its associated files\n",
    "recommendations_per_file = {}\n",
    "for selected_question, files in tqdm(files_questions.items()):\n",
    "    for file in tqdm(files):\n",
    "        conversation_data = load_conversation(file)\n",
    "        conversation = conversation_data[\"messages\"]\n",
    "        student_tutor_conversations = add_role_prefix_to_conversation(conversation)\n",
    "\n",
    "        # Use LLM to get recommendations for each conversation\n",
    "        recommendation = query_llm(conversation_summary, student_tutor_conversations, openai_model=openai_model)\n",
    "\n",
    "        # Store it in the dictionary\n",
    "        if selected_question in recommendations_per_file:\n",
    "            recommendations_per_file[selected_question].append(recommendation)\n",
    "        else:\n",
    "            recommendations_per_file[selected_question] = [recommendation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e8c8a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:39<00:00,  5.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Merge the recommendations into a single list per question\n",
    "recommendations_per_question = {}\n",
    "for selected_question in tqdm(recommendations_per_file.keys()):\n",
    "    recommendations = recommendations_per_file[selected_question]\n",
    "    merged_recommendation = \"\\n\\n\".join(recommendations)\n",
    "\n",
    "    # Use LLM to get merged recommendation for each question.\n",
    "    merged_recommendation = query_llm(question_summary, merged_recommendation, openai_model=openai_model)\n",
    "\n",
    "    recommendations_per_question[selected_question] = merged_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cfc4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overall recommendations\n",
    "merged_overall_recommendations = \" \".join(recommendations_per_question.values())\n",
    "# Use LLM to get overall recommendations\n",
    "overall_recommendations = query_llm(overall_summary, merged_overall_recommendations, openai_model=openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbd7620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recommendations_per_file.pkl', 'wb') as f:\n",
    "    pickle.dump(recommendations_per_file, f)\n",
    "\n",
    "with open('recommendations_per_question.pkl', 'wb') as f:\n",
    "    pickle.dump(recommendations_per_question, f)\n",
    "\n",
    "with open('overall_recommendations.pkl', 'wb') as f:\n",
    "    pickle.dump(overall_recommendations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe1eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('recommendations_per_file.pkl', 'rb') as f:\n",
    "    recommendations_per_file = pickle.load(f)\n",
    "\n",
    "with open('recommendations_per_question.pkl', 'rb') as f:\n",
    "    recommendations_per_question = pickle.load(f)\n",
    "\n",
    "with open('overall_recommendations.pkl', 'rb') as f:\n",
    "    overall_recommendations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d6abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student’s struggles across several topics boil down to three interrelated themes:\n",
      "\n",
      "1. Unclear or mixed‐up definitions  \n",
      "   • Eigenvalues vs. eigenvectors (λ I, A v = λ v, characteristic polynomial)  \n",
      "   • Symmetric matrix (A = Aᵀ, aᵢⱼ = aⱼᵢ) vs. identity or commuting factors  \n",
      "   • Orthogonal vectors vs. orthogonal matrix (MᵀM = I) vs. linear independence  \n",
      "   • Rotation matrix entries (linking cos θ, sin θ to geometric rotation)  \n",
      "\n",
      "2. Gaps in algebraic mechanics and notation  \n",
      "   • Mixing scalars with matrices (when and why λ → λ I)  \n",
      "   • Rearranging/factoring to get (A – λ I)v = 0 and using det(A – λ I)=0  \n",
      "   • Applying the transpose‐of‐a‐product rule ((AB)ᵀ = BᵀAᵀ) and det(Aᵀ)=det(A)  \n",
      "   • Carrying out dot products correctly (sum vs. vector) and checking all pairwise products  \n",
      "   • Tracking dimensions and correct order in matrix multiplication  \n",
      "\n",
      "3. Difficulty linking abstract statements to concrete checks  \n",
      "   • Translating definitions into index or component form (e.g. aᵢⱼ = aⱼᵢ, v·w=0)  \n",
      "   • Working through small examples step by step (2×2 matrices, specific θ for rotations)  \n",
      "   • Articulating each logical step (rather than skipping from premise to conclusion)  \n",
      "   • Recognizing when counterexamples violate hypotheses (e.g. zero vector in an “orthogonal” set)  \n",
      "\n",
      "In short, the student needs clearer, self-generated definitions; guided practice on the algebraic rules (scalar/matrix interplay, transposes, determinants, dot products); and more worked‐out examples that tie abstract properties to entrywise and geometric checks.\n"
     ]
    }
   ],
   "source": [
    "print(overall_recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linearalgebra-vta-43WzCJzv-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
